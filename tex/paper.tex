\documentclass[twocolumn,superscriptaddress,aps]{revtex4-1}

\usepackage[utf8]{inputenc}

\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}

\usepackage{bm}
\usepackage{cancel}
\usepackage{bbold}
\usepackage{slashed}
\usepackage{graphicx}
\usepackage{color}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}

\begin{document}


% ==============================================================================

\title{\Large{Adversarial Training against Systematic Uncertainty}}
\vspace{1cm}
\author{\small{\bf Gilles Louppe}\thanks{\texttt{g.louppe@nyu.edu}}}
\affiliation{New York University}
\author{\small{\bf Michael Kagan}\thanks{\texttt{makagan@slac.stanford.edu}}}
\affiliation{SLAC National Accelerator Laboratory}
\author{\small{\bf Kyle Cranmer}\thanks{\texttt{kyle.cranmer@nyu.edu}}}
\affiliation{New York University}

\begin{abstract}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Etiam commodo, enim
vitae facilisis pretium, ligula justo aliquet lectus, non interdum erat lorem et
nisi. Donec pharetra lectus in magna pellentesque vehicula. Praesent dapibus
lorem sed enim lacinia, a vestibulum sapien mattis. Sed ornare mollis aliquet.
Nulla tempor lacinia tortor, in rhoncus augue porta nec. Morbi sed convallis
nibh, eu hendrerit turpis. Curabitur sit amet rhoncus purus. Curabitur eget
magna lorem. Phasellus pretium nisi quis est tincidunt, faucibus vulputate augue
viverra. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin a urna a
ex egestas pulvinar.

\end{abstract}

\maketitle


% ==============================================================================

\section{Introduction}

The discovery of new particles or physical phenomena in high energy physics
experiments, like those at the Large Hadron Collider, requires the observation
of statistically significant deviations from the predictions of the Standard
Model, the current model of the known fundamental particles and their
interactions.  Typically, discovery requires satisfying the \textit{5-sigma
rule}, whereby the deviation must be at least five standard deviations from
predictions, i.e. having a p-value of $p \lesssim 3 \times 10^{-7}$. The
evaluation of the statistical significance must not only account for statistical
uncertainties, but also systematic uncertainties that represent our incomplete
knowledge of the theory of particle interactions or of the properties of the
experimental detection apparatus. Systematic uncertainties can alter the
expected rate of signal and background classes, e.g. their priors, as well as
the distributions of features. As such, the signal class true-positive rate and
the background class false-positive rate of a classifier can change with
variations in the systematic uncertainty thereby decreasing the sensitivity of
an analysis. In this paper, we present a new strategy for training a classifier,
based on adversarial training~\citep{goodfellow2014generative}, which aims to
build a classifier whose decision function is invariant under variations due to
systematic uncertainties.  In this way, uncertainties on predictions from
simulations will have a reduced impact when computing the significance of
potential deviations after application of the discriminant model and thereby
increase the sensitivity of the analysis to signals from potential new physics
sources. \glnote{Shall we be more explicit somewhere and put a formula
underlining the effect of having a pivotal classifier on the resulting
sensitivity? (versus a classifier whose decision function depends on the
nuisance.)}

Given data $X$ and associated labels $Y$
taking values $y \in {\cal Y} = \{s,
b_{i=1...k} \}$ where $s$ is the signal class and $b_{i=1...k}$ are
the background classes, then the probability density of the data
can be written as a mixture
\begin{equation}
p(X) = \pi_s p(X|Y=s) + \sum_{i=1}^{k} \pi_{b_i} p(X | Y=b_i),
\end{equation}
where $\pi_{y}$ represents the prior for the signal or a given
background, and $p(X|Y)$ is the conditional distribution of the data
for the signal or a given background. Systematic uncertainties can
affect this distribution in the following ways \glnote{add references}:
\begin{itemize}
\item Uncertainties on the priors $\pi_{y}$.  These arise from
  uncertainties on the rates we expect a given process to occur as
  predicted from theoretical calculations or from alternative
  measurements of the rate of a given class.

\item Uncertainties in the simulators of the physical interactions,
  giving rise to uncertainties on the distributions $p(X|Y)$.  These
  uncertainties are often determined by comparing the predictions of
  different simulations using differing models of the same physical
  process.

\item Uncertainties on the detection apparatus and its effect on the
  measurement of particle properties, giving rise to uncertainties on
  the distributions $p(X|Y)$.  For instance, this includes
  uncertainties on our knowledge of the mean and variance of the
  measured energy distribution of a particle.
\end{itemize}
In this paper, we will not address uncertainties of the first type
mentioned above (uncertainties on the priors).  Rather, we will focus
on mitigating the effect of systematic uncertainties that alter the
conditional distributions $p(X|Y)$.

Systematic uncertainties are parameterized by nuisance parameters in the
statistical analysis of data \glnote{add references}.  Nuisance parameter often,
though not always, have a suitable prior coming from alternative measurements in
data.  For instance, The average measured energy of an electron in simulation
may be compared with those in data using a relatively pure sample of $Z$ boson
decays to electrons.  The average measured electron energy can not be measured
without uncertainty, and the measured uncertainty can be used in other analyses
to estimate the uncertainty deriving from our imperfect knowledge of the
electron's energy.  A nuisance parameter is assigned to the electron energy
uncertainty, which controls the variation of the electron energy in the
statistical analysis of data.  Nuisance parameters are typically, though not
always, constrained by a suitable prior, e.g. a Gaussian distribution with a
standard deviation equal to the measured uncertainty. \glnote{... and
accordingly, we want to build a classifier independent on the nuisance
parameter, as a way to be independent of the corresponding systematics.}

\glnote{Add paper outline.}

% ==============================================================================

\section{Problem statement}
\label{sec:problem}

Let assume a probability space $(\Omega, {\cal F}, P)$, where $\Omega$ is a
sample space, ${\cal F}$ is a set of events and $P$ is a probability measure.
Let consider the multivariate random variables $X_z: \Omega \mapsto
\mathbb{R}^p$ and $Y: \Omega \mapsto {\cal Y}$, where $X_z$ denotes a dependence
of the functional $X$ on a nuisance parameter $Z$ whose values $z \in {\cal Z}$  define a
parameterized family of its systematic uncertainties. That is, $X_z$ and $Y$
induce together a joint probability distribution $p(X,Y|z)$, where the
conditional on $z$ denotes $X_z$. For training, let further assume a finite set $\{
x_i, y_i, z_i \}_{i=1}^N$ of realizations $X_{z_i}(\omega_i), Y(\omega_i)$, for
$\omega_i \in \Omega$ and known values $z_i$ of the nuisance parameter. Our goal
is to learn a function $f(\cdot;\theta_f) : \mathbb{R}^p \mapsto {\cal Y}$ of
parameters $\theta_f$ (e.g., a neural network-based classifier if ${\cal Y}$ is
a finite set of classes) and minimizing  a loss ${\cal L}_f(\theta_f)$ (e.g.,
the cross-entropy). In addition, we require that $f(X_z ; \theta_f)$ should be
robust to the value $z$ of the nuisance parameter  -- which remains unknown at
test time. More specifically, we aim at building $f$ such that in the ideal case
\begin{equation}\label{eqn:criterion-true}
f(X_{z}(\omega) ; \theta_f) = f(X_{z^\prime}(\omega) ; \theta_f)
\end{equation} for all
samples $\omega \in \Omega$ and all $z, z^\prime$ pairs of values of the
nuisance parameter.

Since we do not have training tuples $(X_{z}(\omega),
X_{z^\prime}(\omega))$ (for the same unknown $\omega$), we propose instead to
solve the closely related problem of finding a predictive function $f$ such that
\begin{align}\label{eqn:criterion-measure}
    &P(\{ \omega | f(X_{z}(\omega) ; \theta_f) = y \} ) \nonumber \\
    &\indent = P( \{ \omega' | f(X_{z^\prime}(\omega') ; \theta_f) = y\})
\end{align}
for all $y \in {\cal Y}$. In words, we are looking for a predictive function $f$
which is a pivotal quantity \citep{degroot1986probability} with respect to the
nuisance parameter. That is, such that  the distribution of $f(X_z; \theta_f)$
is invariant with respect to the value $z$ of the nuisance. Note that a function
$f$ for which Eqn.~\ref{eqn:criterion-true} is true necessarily satisfies
Eqn.~\ref{eqn:criterion-measure}. In general, the converse is however not true,
since the sets of samples $\{ \omega | f(X_{z}(\omega); \theta_f) = y \}$ and $\{
\omega' | f(X_{z^\prime}(\omega'); \theta_f) = y \}$ do not need to be the same
for the equality to hold. In order to simplify notations, and as only
Eqn.~\ref{eqn:criterion-measure} is of direct interest in this work, we denote
from here on the pivotal quantity criterion as
\begin{equation}\label{eqn:criterion}
    p(f(X ; \theta_f) | z ) = p(f(X ; \theta_f) | z^\prime )
\end{equation}
for all $z,z^\prime \in  {\cal Z}$.


% ==============================================================================

\section{Method}
\label{sec:method}

Adversarial training was first proposed by \citep{goodfellow2014generative} as a
way to build a generative model capable of producing samples from random noise
$z \sim p_Z$. More specifically, the authors pit a generative model $g:
\mathbb{R} \mapsto \mathbb{R}^p$ against an adversary classifier $d :
\mathbb{R}^p \mapsto \{ 0, 1\}$ whose antagonistic objective is to recognize
real data $X$ from generated data $g(Z)$. Both models $g$ and $d$ are trained
simultaneously, in such a way that $g$ learns to produce samples that are
difficult to identify by $d$, while $d$ incrementally adapts to changes in $g$.
At the equilibrium, $g$ models a distribution whose samples can be identified by
$d$ only by chance. That is, assuming enough capacity in $d$ and  $g$, the
distribution $p_{g(Z)}$ eventually converges towards the real distribution
$p_X$.

\tikzstyle{every node}=[font=\scriptsize]
\begin{figure*}
    \usetikzlibrary{arrows}
    \def\layersep{1cm}

    \begin{tikzpicture}[shorten >= 1pt, ->, node distance=\layersep,scale=.85]
    \tikzstyle{neuron} = [circle, minimum size=0.25cm, draw=black!30, line width=0.3mm, fill=white]

    % Classifier f
    \node at (2,0) {Classifier $f$};
    \draw (-1,-0.5) rectangle (4,-5.5);

    \path[->, shorten >= 0pt] (-2,-3) edge (-1,-3);
    \node[left] at (-2,-3) {$X$};

    \path[-o, shorten >= 0pt] (1.5,-6.5) edge (1.5,-5.5);
    \node[below] at (1.5,-6.5) {$\theta_f$};

    \path[->, shorten >= 0pt] (3.5,-3) edge (6.5,-3);
    \node[above] at (5.25,-3) {$f(X;\theta_f)$};

    \path[dashed,-] (5.25,-3) edge (5.25,-6.5);
    \node[below] at (5.25,-6.5) {${\cal L}_f(\theta_f)$};

    \foreach \name / \y in {1,...,3}
        \node[neuron] (f-I-\name) at (-0.5,-1-\y) {};

    \foreach \name / \y in {1,...,5}
        \node[neuron] (f-H1-\name) at (-0.5cm+\layersep,-\y cm) {};
    \foreach \name / \y in {1,...,5}
        \node[neuron] (f-H2-\name) at (-0.5cm+3*\layersep,-\y cm) {};

    \node[neuron] (f-O) at (-0.5cm+4*\layersep,-3cm) {};

    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,5}
            \path[black!20] (f-I-\source) edge (f-H1-\dest);

    \foreach \source in {1,...,5}
        \path[black!30] (f-H2-\source) edge (f-O);

    \node[black!30] at (1.5,-3) {...};

    % Adversary r
    \node at (11.75,0) {Adversary $r$};
    \draw (6.5,-0.5) rectangle (11.5,-5.5);

    \node[above] at (12.75,-2) {$\theta^1(f(X;\theta_f);\theta_r)$};
    \path[-o, shorten >= 0pt] (11,-2.0) edge (14,-2.0);
    \node[above] at (12.75,-3) {$\theta^2(f(X;\theta_f);\theta_r)$};
    \path[-o, shorten >= 0pt] (11,-3) edge (14,-3);
    \node[above] at (12.75,-4) {$\dots$};
    \path[-o, shorten >= 0pt] (11,-4) edge (14,-4);

    \path[-o, shorten >= 0pt] (9,-6.5) edge (9,-5.5);
    \node[below] at (9,-6.5) {$\theta_r$};

    \foreach \name / \y in {1,...,1}
        \node[neuron] (r-I-\name) at (7,-2-\y) {};

    \foreach \name / \y in {1,...,5}
        \node[neuron] (r-H1-\name) at (7cm+\layersep,-\y cm) {};
    \foreach \name / \y in {1,...,5}
        \node[neuron] (r-H2-\name) at (7cm+3*\layersep,-\y cm) {};

    \node[neuron] (r-O1) at (7cm+4*\layersep,-2cm) {};
    \node[neuron] (r-O2) at (7cm+4*\layersep,-3cm) {};
    \node[neuron] (r-O3) at (7cm+4*\layersep,-4cm) {};

    \foreach \source in {1,...,1}
        \foreach \dest in {1,...,5}
            \path[black!20] (r-I-\source) edge (r-H1-\dest);

    \foreach \source in {1,...,5}
        \path[black!30] (r-H2-\source) edge (r-O1);
    \foreach \source in {1,...,5}
        \path[black!30] (r-H2-\source) edge (r-O2);
    \foreach \source in {1,...,5}
        \path[black!30] (r-H2-\source) edge (r-O3);

    \node[black!30] at (9,-3) {...};

    \draw (14,-1.5) rectangle (17,-4.5);
    \path[->, shorten >= 0pt] (15.5,-0.5) edge (15.5,-1.5);
    \node[above] at (15.5,-0.5) {$Z$};
    \path[->, shorten >= 0pt] (15.5,-4.5) edge (15.5,-5.5);
    \node[below] at (15.5,-5.5) {$p_{\theta_r}(Z|f(X;\theta_f))$};
    \node at (15.5,-3) {${\cal P}(\theta^1, \theta^2, \dots)$};

    \draw[dashed,-] (15.5,-5) -| (12.75,-6.5);
    \node[below] at (12.75,-6.5) {${\cal L}_r(\theta_f, \theta_r)$};

    \end{tikzpicture}

    \caption{Architecture for the adversarial training of a binary classifier
    $f$ against a continuous parameter $Z$.}
    \label{fig:architecture}
    \end{figure*}

In this work, we repurpose adversarial training as a means to constraint the
predictive model $f$ in order to satisfy Eqn.~\ref{eqn:criterion}. As
illustrated in Figure~\ref{fig:architecture}, we pit $f$ against an adversary
model $r := p_{\theta_r}(z | f(X;\theta_f))$ of parameters $\theta_r$ and
associated loss ${\cal L}_r(\theta_f, \theta_r)$. This model takes  as input
realizations of $f(X; \theta_f)$, for the current value $\theta_f$ of $f$
parameters, and produces as output a function $p_{\theta_r}(z | f(X;\theta_f))$
modeling the posterior probability density that $z$ parameterizes the sample $X$
observed through $f(\cdot;\theta_f)$.
Intuitively, if $p(f(X; \theta_f)|z)$ varies with $z$,
then the corresponding correlation can be captured by $r$. By contrast, if
$p(f(X; \theta_f)|z)$ is invariant with $z$, as we require, then $r$ should
perform poorly and be close to random guessing. Training $f$ such that it
additionally minimizes the performance of $r$ therefore acts as a regularization
towards Eqn.~\ref{eqn:criterion}.

If $Z$ takes discrete values, then $p_{\theta_r}$ can be represented e.g. as a
probabilistic classifier $\mathbb{R} \mapsto \mathbb{R}^{|{\cal Z|}}$ whose
output $j$ (for $j=1, \dots, |{\cal Z}|$) is the estimated probability mass
$p_{\theta_r}(z_j|f(X;\theta_f))$. Similarly, if $Z$ takes continuous values and
if we assume some parameteric distribution ${\cal P}$ for $Z|f(X;\theta_f)$ (e.g., a
truncated gaussian over a bounded support), then $p_{\theta_r}$ can be
represented e.g. as network whose output $j$ is the estimated value of the
corresponding parameter $\theta^j$ of that distribution (e.g., its mean and variance). As
in \citep{nix1994estimating}, the estimated probability density
$p_{\theta_r}(z|f(X;\theta_f))$ can then be evaluated for any $z \in {\cal Z}$.
As further explained in the next section, let us note that the adversary $r$ may
take any form, i.e. it does need to be a neural network, as long as it exposes a
differentiable function $p_{\theta_r}(z|f(X;\theta_f))$ of sufficient capacity
to represent the true distribution within its bounded support.

As for generative adversarial networks, we propose to
train $f$ and $r$ simultaneously, which we carry out by considering
the value function
\begin{equation}
    E(\theta_f, \theta_r) = {\cal L}_f(\theta_f) - {\cal L}_r(\theta_f, \theta_r)
\end{equation}
that we optimize by finding the saddle point $(\smash{\hat\theta_f}, \smash{\hat\theta_r})$ such that
\begin{align}
    \smash{\hat\theta_f} &= \arg \min_{\theta_f} E(\theta_f, \smash{\hat\theta_r}), \label{eqn:min_thetaf} \\
    \smash{\hat\theta_r} &= \arg \max_{\theta_r} E(\smash{\hat\theta_f}, \theta_r) \label{eqn:max_thetar}.
\end{align}
Without loss of generality, the adversarial training procedure to obtain
$(\smash{\hat\theta_f}, \smash{\hat\theta_r})$ is formally presented in
Algorithm~\ref{alg:adversarial-training} in the case of a binary classifier $f :
\mathbb{R}^p \mapsto [0,1]$ modeling $p(Y=1|X)$. For reasons further explained
in Section~\ref{sec:theory}, ${\cal L}_f$ and ${\cal L}_r$  are respectively set to the
expected value of the
negative log-likelihood of $Y|X$ under $f$ and of $Z|f(X;\theta_f)$ under
$r$:
\begin{align}
    {\cal L}_f(\theta_f) &= \mathbb{E}_{X} \mathbb{E}_{Y|X} [ -\log p_{\theta_f} (Y|X) ], \\
    {\cal L}_r(\theta_f, \theta_r) &= \mathbb{E}_{X}  \mathbb{E}_{Z|f(X;\theta_f)} [-\log p_{\theta_r} (Z|f(X;\theta_f))].
\end{align}
The optimization algorithm consists in using stochastic gradient descent
alternatively for solving Eqn.~\ref{eqn:min_thetaf} and \ref{eqn:max_thetar}.

\begin{figure*}
    \begin{minipage}{\linewidth}
    \begin{algorithm}[H]
    \caption{Adversarial training of a classifier $f$ against an adversary $r$.}

    \begin{flushleft}
        {\it Inputs:} training data $\{ x_i, y_i, z_i \}_{i=1}^N$;\\
        {\it Outputs:} $\smash{\hat\theta_f}, \smash{\hat\theta_r}$;\\
        {\it Hyper-parameters:} Number $T$ of training iterations,
                                Number $K$ of gradient steps to update $r$.
    \end{flushleft}

    \label{alg:adversarial-training}
    \begin{algorithmic}[1]
        \For{$t=1$ to $T$}
            \For{$k=1$ to $K$} \Comment{Update $r$}
                \State{Sample minibatch $\{x_m, z_m \}_{m=1}^M$} of size $M$;
                \State{With $\theta_f$ fixed, update $r$ by ascending its stochastic gradient $\nabla_{\theta_r} E(\theta_f, \theta_r) :=$
                $$\nabla_{\theta_r} \sum_{m=1}^M \log p_{\theta_r}(z_m|f(x_m;\theta_f))  ;$$}
            \EndFor
            \State{Sample minibatch $\{x_m, y_m, z_m \}_{m=1}^M$} of size $M$; \Comment{Update $f$}
            \State{With $\theta_r$ fixed, update $f$ by descending its stochastic gradient $\nabla_{\theta_f} E(\theta_f, \theta_r) :=$
            $$\nabla_{\theta_f}  \sum_{m=1}^M \left[ -\log p_{\theta_f}(y_m|x_m)  +\log p_{\theta_r}(z_m|f(x_m;\theta_f))  \right],$$
            \indent where $p_{\theta_f}(y_m|x_m)$ denotes $1(y_m=0)(1-f(x_m;\theta_f)) + 1(y_m=1)f(x_m;\theta_f))$;}
        \EndFor
    \end{algorithmic}
    \end{algorithm}
    \end{minipage}
\end{figure*}


% ==============================================================================

\section{Theoretical results}
\label{sec:theory}

In this section, we show that in the setting of
Algorithm~\ref{alg:adversarial-training} where ${\cal L}_f$ and ${\cal L}_r$ are
respectively set to expected value of the negative log-likelihood of $Y|X$ under
$f$ and of $Z|f(X;\theta_f)$ under $r$, the procedure converges to a classifier
$f$ which is a pivotal quantity in the sense of Eqn.~\ref{eqn:criterion}.

In this setting, the nuisance parameter $Z$ is considered as a random variable
of bounded support, for which we require the uniform prior $p(z)$ (for $z \in
{\cal Z}$). Importantly, classification of $Y$ with respect to $X$ is therefore
considered in the context where $Z$ is marginalized out, which means that the
classifier minimizing ${\cal L}_f$ is optimal with respect to $Y|X$, but not
necessarily with $Y|X,Z$. Results hold for a nuisance parameter $Z$ taking
either categorical values or continuous values within a bounded support. By
abuse of notation, $H(p_Z)$ denotes the differential entropy in this latter
case. Finally, propositions below are derived in a non-parametric setting, by
assuming that both $f$ and $r$ have enough capacity.

\begin{proposition}\label{prop:1}
Let $\hat\theta_r = \arg \max_{\theta_r} E(\theta_f,
\theta_r)$ for fixed $\theta_f$. If $p_{\hat\theta_r}(z|f(X;\theta_f)) = p(z)$
for all $z \in {\cal Z}$ and all values of $f(X;\theta_f)$, then $f$ is a pivotal quantity.
\end{proposition}

\begin{proof}
Since we assume enough capacity for $r$, the optimal parameters $$\smash{\hat\theta}_r =
\arg \max_{\theta_r} E(\theta_f, \theta_r) = \arg \min_{\theta_r} {\cal L}_r(\theta_f,
\theta_r)$$ are such that $p_{\hat\theta_r}(z|f(X;\theta_f)) = p(z|f(X;\theta_f))$.
By assumption, $p_{\hat\theta_r}(z|f(X;\theta_f)) = p(z)$,
and therefore $p(z|f(X;\theta_f)) = p(z)$.
Using the Bayes' rule, we write
\begin{align*}
    p(f(X;\theta_f)|z) &= \frac{ p(z|f(X;\theta_f)) p(f(X;\theta_f)) } { p(z)}\\
                       &= p(f(X;\theta_f)),
\end{align*}
which holds for all $z \in {\cal Z}$ and all values of $f(X;\theta_f)$ and implies that $f$ is a pivotal quantity.
\end{proof}

\begin{proposition}\label{prop:2}
If there exists a saddle point $(\smash{\hat\theta}_f, \smash{\hat\theta}_r)$
for Eqn.~\ref{eqn:min_thetaf} and \ref{eqn:max_thetar} such that
$E(\hat\theta_f, \hat\theta_r) = \mathbb{E}_{X} [H(p_{Y|X}) - H(p_Z)]$, then
$f(\cdot;\smash{\hat\theta}_f)$ is both an optimal classifier and a pivotal
quantity.
\end{proposition}

\begin{proof}

For fixed $\theta_f$, the adversary $r$ is optimal at $\hat\theta_r = \arg
\max_{\theta_r} E(\theta_f, \theta_r)  = \arg \min_{\theta_r} {\cal
L}_r(\theta_f, \theta_r)$, in which case $p_{\hat\theta_r}(z|f(X;\theta_f)) =
p(z|f(X;\theta_f))$ and ${\cal L}_r$ reduces to the expected entropy
$\mathbb{E}_{X} [ H(p_{Z|f(X;\theta_f)}) ]$ of the conditional distribution of the nuisance. The
value function $E$ can therefore be rewritten as $$E'(\theta_f) = {\cal
L}_f(\theta_f) - \mathbb{E}_{X} [ H(p_{Z|f(X;\theta_f)}) ].$$  In particular, we have the lower
bound $$\mathbb{E}_{X} [ H(p_{Y|X}) - H(p_Z)] \leq {\cal L}_f(\theta_f) - \mathbb{E}_{X} [H(p_{Z|f(X;\theta_f)})]$$
where the equality holds at $\smash{\hat\theta}_f = \arg \min_{\theta_f}
E'(\theta_f)$ only when
\begin{itemize}
    \item $\smash{\hat\theta}_f$ minimizes the negative log-likelihood ${\cal
    L}_f$ of $X|Y$, which happens when $\smash{\hat\theta}_f$ are the parameters
    of an optimal classifier and in which case ${\cal L}_f$ reduces to its
    minimum value $\mathbb{E}_{X} [H(p_{Y|X})]$,

    \item $\smash{\hat\theta}_f$ maximimizes the expected entropy
    $\mathbb{E}_{X} [H(p_{Z|f(X;\theta_f)})]$, which happens when all outcomes of
    $Z|f(X;\smash{\hat\theta}_f)$ are equally likely. In other words,
    $p(z|f(X;\smash{\hat\theta}_f)) = p(z)$ for all $z \in {\cal Z}$ and all values of $f(X;\theta_f)$ since we
    require a uniform prior by construction. Note that in the continuous case,
    the supremum of the differential entropy over continuous distributions on
    the same bounded support is also realized by the uniform distribution over
    that support.
\end{itemize}
When the lower bound is active, we have $p_{\theta_r}(z|f(X;\theta_f)) = p(z)$
because of the second condition, which induces that the
optimal classifier $f(\cdot;\smash{\hat\theta}_f)$ is also a pivotal
quantity as a consequence of Proposition~\ref{prop:1}.

\end{proof}

Proposition~\ref{prop:2} suggests that if at each step of
Algorithm~\ref{alg:adversarial-training} the adversary $r$ is allowed to reach
its optimum given $f$ (e.g., by setting $K$ sufficiently high) and if $f$ is
updated to improve ${\cal L}_f(\theta_f) - \mathbb{E}_X [ H(p_{Z|f(X;\theta_f)}) ]$, then $f$
should converge to a classifier which is both optimal and pivotal, provided such
a classifier exists. On many problems of interest though, such a classifier may
not exist because the nuisance parameter directly shapes the decision boundary,
in which cases the lower bound $\mathbb{E}_{X} [ H(p_{Y|X}) - H(p_Z)] < {\cal L}_f(\theta_f) - \mathbb{E}_{X} [H(p_{Z|f(X;\theta_f)})]$ is strict: $f$ can either be an optimal classifier or a
pivotal quantity, but not both simultaneously. In this situation, it is natural
to rewrite the value function $E$  as
\begin{equation}
    E_\lambda(\theta_f, \theta_r) = {\cal L}_f(\theta_f) - \lambda {\cal L}_r(\theta_f, \theta_r),
\end{equation}
where $\lambda \geq 0$ is a hyper-parameter controlling the trade-off between
the performance of $f$ and its independence with respect to the nuisance
parameter. Setting $\lambda$ to a large value will preferably enforces $f$ to
be pivotal while setting $\lambda$ close to $0$ will rather constraint $f$ to be
optimal.

Interestingly, let us emphasize that these results hold using only the (1D) output
of $f$ (in the case of binary classification) as input to the adversary. We
could similarly enforce an intermediate representation of the data to be
pivotal, e.g. as in \citep{ganin2014unsupervised}, but this is in fact not
necessary.


% ==============================================================================

\section{Experiments}

\subsection{Toy example}

As an illustrating example, let us consider...

\glnote{Define the architecture of $r$ when $Z$ is categorical ($r$ is a
standard classifier) or continuous ($r$ is e.g a 2-output NN such that the
distribution of $Z|f(X)$ is modeled by a truncated Gaussian of known support). Cite Nix.}

\subsection{Physics example}


% ==============================================================================

\section{Related work}

To account for systematic uncertainties, experimentalists in high energy physics
typically take as fixed a classifier $f$ built from training data for a nominal
value $z_0$ of the nuisance parameter, and then propagate uncertainty
\glnote{add ref} by estimating $p(f(x)|z)$ with a parameterized calibration
procedure. Clearly, this classifier is however not optimal for $z \neq z_0$. In
this setting, parameterized
classifiers~\citep{cranmer2015approximating,Baldi:2016fzo} directly take
(nuisance) parameters as additional input variables, hence ultimately providing
the most statistically powerful approach for incorporating the effect of
systematics on the underlying classification task.  As argued in
\citep{Neal:2007zz}, such classifiers can however not be used on real data since
the correct value $z$ of the nuisance often remains unknown. This is typically
not an issue in the context of parameter
inference~\citep{cranmer2015approximating}, where nuisance parameters are
marginalized out, but otherwise often limits the range of their applications. In
practice, parameterized classifiers  are also computationally expensive to build
and evaluate. In particular, calibrating their decision function, i.e.
approximating $p(f(x,z)|z)$ as a continuous function of $z$, remains an open
challenge. By contrast, constraining $f$ to be pivotal yields a classifier which
may not be optimal with respect to $Y|X,Z$, as discussed in
Section~\ref{sec:theory}, but that can otherwise be used in a wider range of
applications, since knowing the correct value $z$ of the nuisance is not
necessary. Similarly, calibration needs to be carried out only once, since  the
dependence on the nuisance is now built-in. \glnote{Shall we also discuss the
relation with \citep{Neal:2007zz} where point estimates of the nuisance are used
as inputs to $f$?}

In machine learning, learning a pivotal quantity can be related to the problem
of domain adaptation
\citep{blitzer2006domain,pan2011domain,gopalan2011domain,gong2013connecting,baktashmotlagh2013unsupervised,ganin2014unsupervised},
where the goal is often stated as trying to learn a domain-invariant
representation of the data. Likewise, our method also relates to the problem of
enforcing fairness in classification \citep{zemel2013learning,EdwardsS15}, which
is stated as learning a classifier that is independent of some chosen attribute
such as gender, color or age. For both families of methods, the problem can
equivalently be stated as learning a classifier which is a pivotal quantity with
respect to either the domain or the selected feature. In this context,
\citep{ganin2014unsupervised,EdwardsS15} are certainly among the closest to our
work, in which domain invariance and fairness are enforced through an
adversarial minimax setup composed of a classifier and an adversary
discriminator. Following this line of work, our method can be regarded as a
generalization that also supports the continuous case, which can be viewed as
handling infinitely many domains, provided they can be continuously
parameterized, or as enforcing fairness over continuous attributes.

\glnote{Check related work of cited references to see if important related work are missing.}


% ==============================================================================

\section{Conclusions}


% ==============================================================================

\begin{acknowledgments}
\end{acknowledgments}


% ==============================================================================

\bibliography{bibliography.bib}

\end{document}
