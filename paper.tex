\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{color}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}

\title{Adversarial Training of Neural Networks\\
against Systematic Uncertainty}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Gilles Louppe \\
  New York University\\
  \texttt{g.louppe@nyu.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

\glnote{Distinction between statistical and systematic uncertainty.}
\glnote{Define nuisance parameters.}
\glnote{We want to build an accurate classifier whose output remains invariant with
respect to systematic uncertainties.}
% https://www.slac.stanford.edu/econf/C030908/papers/TUAT004.pdf
\glnote{Motivate the criterion: allow to derive guarantees no matter $\lambda$,
while $f$ may otherwise be good or very bad depending on the nuisance if trained
on the mixture. E.g. to form hypothesis to be later confirmed by data.}



\section{Problem statement}
\label{sec:problem}

Let assume a probability space $(\Omega, {\cal F}, P)$, where $\Omega$ is a
sample space, ${\cal F}$ is a set of events and $P$ is a probability measure.
Let consider the multivariate random variables $X_\lambda: \Omega \mapsto
\mathbb{R}^p$ and $Y: \Omega \mapsto {\cal Y}$, where $X_\lambda$ depends on a
nuisance parameter $\lambda$ whose values define the family of its systematic uncertainties.
That is, $X_\lambda$ and $Y$ induce together
a joint probability distribution $p(X,Y|\lambda)$, where the
conditional on $\lambda$ denotes $X_\lambda$. For training, let further assume a
finite set $\{ x_i, y_i, \lambda_i \}_{i=1}^N$ of realizations
$X_{\lambda_i}(\omega_i), Y(\omega_i)$, for $\omega_i \in \Omega$ and known
values $\lambda_i$ of the nuisance parameter. Our goal is to learn a function $f :
\mathbb{R}^p \mapsto {\cal Y}$ (e.g., a classifier if ${\cal Y}$ is a finite set
of classes) minimizing the expected value of a loss $L(Y, f(X_{\lambda}))$, with
the constraint that $f(X_\lambda)$ should be robust to the value of the nuisance
parameter $\lambda$ -- which remains unknown at test time. More specifically, we
aim at building $f$ such that in the ideal case
\begin{equation}\label{eqn:criterion-true}
f(X_{\lambda_i}(\omega)) = f(X_{\lambda_j}(\omega))
\end{equation} for any
sample $\omega \in \Omega$ and any $\lambda_i, \lambda_j$ pair of values of the
nuisance parameter.

Since we do not have training tuples $(X_{\lambda_i}(\omega),
X_{\lambda_j}(\omega))$ (for the same unknown $\omega$), we propose instead to
solve the closely related problem of finding a predictive function $f$ such that
\begin{equation}\label{eqn:criterion}
    P(\{ \omega | f(X_{\lambda_i}(\omega)) = y \} ) = P( \{ \omega' | f(X_{\lambda_j}(\omega')) = y\}) \text{ for all $y \in {\cal Y}$}.
\end{equation}
In words, we are looking for a predictive function $f$ such that  the
distribution of $f(X_\lambda)$ is invariant with respect to the nuisance
parameter $\lambda$. Note that a function $f$ for which Eqn.~\ref{eqn:criterion-true} is
true necessarily satisfies Eqn.~\ref{eqn:criterion}. The converse is however in
general not true, since the sets of samples $\{ \omega | f(X_{\lambda_i}(\omega)) = y \}$
and $\{ \omega' | f(X_{\lambda_j}(\omega')) = y \}$ do not need to be the same
for the equality to hold.


\section{Method}
\label{sec:method}

Adversarial training was first proposed by \cite{goodfellow2014generative} as a
way to build a generative model capable of producing samples from random noise
$z \sim p_Z$. More specifically, the authors pit a generative model $G$ against
an adversary classifier $D$ whose repelling objective is to recognize real from
generated data. Both models $G$ and $D$ are trained simultaneously, in such a
way that $G$ learns to produce samples that are difficult to identify by $D$,
while $D$ incrementally adapts to changes in $G$. At the equilibrium, $G$ models
a distribution whose samples can be identified by $D$ only by chance. In other
words, assuming enough capacity in $D$ and  $G$, the distribution $p_{G(Z)}$
eventually converges towards the real distribution $p_X$.

In this work, we repurpose adversarial training for regularizing the
construction of the predictive model $f$ so as to satisfy
Eqn.~\ref{eqn:criterion}.

\glnote{describe baseline}
\glnote{describe adversarial approach}
\glnote{proof that it solves Eqn. 2}

\section{Experiments}

\section{Related work}

\glnote{Similar to domain adaptation, but with infinitely many domains,
as parameterized by $\lambda$.}

\section{Conclusions}

\subsubsection*{Acknowledgments}

\bibliographystyle{ieeetr}
{\small
\bibliography{bibliography.bib}}

\end{document}
