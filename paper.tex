\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{color}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}

\title{Adversarial Training of Neural Networks\\
against Systematic Uncertainty}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Gilles Louppe \\
  New York University\\
  \texttt{g.louppe@nyu.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}

\glnote{Distinction between statistical and systematic uncertainty.}
\glnote{Define nuisance parameters.}
\glnote{We want to build an accurate classifier whose output remains invariant with
respect to systematic uncertainties.}
% https://www.slac.stanford.edu/econf/C030908/papers/TUAT004.pdf


\section{Problem statement}
\label{sec:problem}

Let assume a probability space $(\Omega, {\cal F}, P)$, where $\Omega$ is a
sample space, ${\cal F}$ is a set of events and $P$ is a probability measure.
Let consider the multivariate random variables $X_\lambda: \Omega \mapsto \mathbb{R}^p$
and $Y: \Omega \mapsto {\cal Y}$, where $X_\lambda$ depends on a nuisance
parameter $\lambda$ encoding for systematic uncertainties in its probability
distribution. That is, $X_\lambda$ and $Y$ induce together the joint probability
distribution $p(X_\lambda,Y)$. For training, let further assume a finite set $\{
 \lambda_i, x_i, y_i, \}_{i=1}^N$ of realizations $X_{\lambda_i}(\omega_i),
Y(\omega_i)$, for $\omega_i \in \Omega$ and known values $\lambda_i$ of the
nuisance parameter. Our goal is to learn a function $f : \mathbb{R}^p \mapsto
{\cal Y}$ (e.g., a classifier if ${\cal Y}$ is a finite set of classes)
minimizing the expected value of a loss $L(Y, f(X_{\lambda}))$, with the constraint that
$f(X_\lambda)$ should be robust to the value of the nuisance parameter
$\lambda$ -- which remains unknown at test time. More specifically, we aim at building $f$ such that in the ideal case
\begin{equation}\label{eqn1}
f(X_{\lambda_i}(\omega)) = f(X_{\lambda_j}(\omega))
\end{equation}
for any sample $\omega \in
\Omega$ and any $\lambda_i, \lambda_j$ pair of values of the nuisance parameter.

Since we do not have training tuples $(X_{\lambda_i}(\omega),
X_{\lambda_j}(\omega))$ (for the same unknown $\omega$), we propose instead to
solve the closely related problem of finding a predictive function $f$ such that
\begin{equation}\label{eqn2}
    P(\{ \omega | f(X_{\lambda_i}(\omega)) = y \} ) = P( \{ \omega' | f(X_{\lambda_j}(\omega')) = y\}) \text{ for all $y \in {\cal Y}$}.
\end{equation}
In words, we are looking for a predictive function $f$ such that  the
distribution of $f(X_\lambda)$ is invariant with respect to the nuisance
parameter $\lambda$. Note that a function $f$ for which Eqn.~\ref{eqn1} is
true necessarily satisfies Eqn.~\ref{eqn2}. The converse is however in
general not true, since the sets of samples $\{ \omega | f(X_{\lambda_i}(\omega)) = y \}$
and $\{ \omega' | f(X_{\lambda_j}(\omega)) = y \}$ do not need to be the same
for the equality to hold. \glnote{Yet, in practice, this criterion is often good enough.}

\section{Method}
\label{sec:method}

\section{Experiments}

\section{Related work}

\glnote{Similar to domain adaptation, but with infinitely many domains,
as parameterized by $\lambda$.}

\section{Conclusions}

\subsubsection*{Acknowledgments}

\bibliographystyle{ieeetr}
{\small
\bibliography{bibliography.bib}}

\end{document}
