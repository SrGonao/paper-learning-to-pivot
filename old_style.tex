% Template for the submission to:
%   The Annals of Probability           [aop]
%   The Annals of Applied Probability   [aap]
%   The Annals of Statistics            [aos]
%   The Annals of Applied Statistics    [aoas]
%   Stochastic Systems                  [ssy]
%
%Author: In this template, the places where you need to add information
%        (or delete line) are indicated by {???}.  Mostly the information
%        required is obvious, but some explanations are given in lines starting
%Author:
%All other lines should be ignored.  After editing, there should be
%no instances of ??? after this line.

% use option [preprint] to remove info line at bottom
% journal options: aop,aap,aos,aoas,ssy
% natbib option: authoryear
\documentclass[aoas,preprint,authoryear]{imsart}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\newcommand{\glnote}[1]{\textcolor{red}{[GL: #1]}}

\begin{document}

\begin{frontmatter}

\title{Learning to Transfer with Semi-Supervised Triply Adversarial Nets}

% indicate corresponding author with \corref{}
% \author{\fnms{John} \snm{Smith}\corref{}\ead[label=e1]{smith@foo.com}\thanksref{t1}}
% \thankstext{t1}{Thanks to somebody}
% \address{line 1\\ line 2\\ printead{e1}}
% \affiliation{Some University}

\author{\fnms{Gilles} \snm{Louppe}}
\affiliation{New York University}

\begin{abstract}

In classification, transfer learning (or its variants known as covariate shift
or domain adaptation) arises whenever target instances are governed by a
distribution that may be arbitrarily different from the distribution of the
source instances. This problem has traditionally been solved by reweighting
approaches or by learning robust representations over domains. In this work, we
propose a new paradigm based on the assumption that the covariate shift is
caused by the use of a different representation of the same underlying objects.
Accordingly, we propose to learn how to transform source instances into target
instances, possibly across input spaces of distinct dimensions, structures or
supports. For this purpose, we extend the generative adversarial networks
framework of \cite{goodfellow2014generative} to a triply adversarial process: a
transformer network $T$ for generating target instances from source instances, a
discriminative network $D$ for separating transformed source instances from
actual target instances, and a classifier network $C \circ T$ for classifying source
instances in the projected space. This 3-player game results in a network $T$
capable of transforming source into target instances, while preserving
separation between classes as enabled by $C$ in the adversarial setup.
Experiments demonstrate the potential of this novative approach, with promising
results when the construction of $C$ can be bootstrapped in a semi-supervised
way  from a few labeled instances from the target space.

\end{abstract}

\end{frontmatter}

\section{Introduction}

% - transfer learning has traditionally been solved through density ratio reweighting
% - works fine, but has the big issue of not working if the support of the source density does not cover the target density support\
%   support is needed for reweighting, but not here (Gretton et al) we can even have different space

% - in this work, we propose instead a new paradigm for solving transfer learning
% - build a generative model for transforming samples from p1 to reproduce p0
% - adversarial learning from Goodfellow, with the practical difference that we often have only a finite set of samples (we need to avoid learning a lookup table)

% other view point: start from dual adversaries, then add to the mix C is a regularizer
% - ensure p(y|t(x)) ~ p(y|x) by having a third classifier in the mix
% issue: T is not unique, how make sure samples are mapped correctly?

% - application: fix simulated data

% Ganin: domains share the same input space, we dont necessarily have to
%        seek a common robust representation, here we seek instead to transform training into test

\section{Method}

% framework
% doubly adversarial network
% T is not unique => triply adversarial + regularization
% proofs?

% we should state that it makes sense only when the same object (\in Omega) can exists in both space, or when there is a suitable transformation from one to the other
% semi-supervised transfer learning: use seeds from the target space to bootstrap C

\section{Experiments}

% - regularization
% - network architecture (propagate x, similar to highway networks)

\section{Related work}

% also related to multi-view learning

\section{Conclusions}

\bibliographystyle{apalike}
\bibliography{bibliography.bib}

\end{document}
